{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code not give full snake game but the deep Q-learning renforcment learnig part full the other python code for full application"
      ],
      "metadata": {
        "id": "roQN0iAKmVMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "LQbhpPH-mSLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qT9uZDuglUS"
      },
      "outputs": [],
      "source": [
        "class Linear_QNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        " \n",
        "    def save(self, file_name='model_name.pth'):\n",
        "        model_folder_path = 'Path'\n",
        "        file_name = os.path.join(model_folder_path, file_name)\n",
        "        torch.save(self.state_dict(), file_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QTrainer:\n",
        "    def __init__(self,model,lr,gamma):\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.model = model\n",
        "        self.optimer = optim.Adam(model.parameters(),lr = self.lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        " \n",
        "     \n",
        "    def train_step(self,state,action,reward,next_state,done):\n",
        "        state = torch.tensor(state,dtype=torch.float)\n",
        "        next_state = torch.tensor(next_state,dtype=torch.float)\n",
        "        action = torch.tensor(action,dtype=torch.long)\n",
        "        reward = torch.tensor(reward,dtype=torch.float)\n",
        "\n",
        "        if(len(state.shape) == 1):\n",
        "            #(1, x)\n",
        "            state = torch.unsqueeze(state,0)\n",
        "            next_state = torch.unsqueeze(next_state,0)\n",
        "            action = torch.unsqueeze(action,0)\n",
        "            reward = torch.unsqueeze(reward,0)\n",
        "            done = (done, )\n",
        " \n",
        "        pred = self.model(state)\n",
        "        target = pred.clone()\n",
        "        for idx in range(len(done)):\n",
        "            Q_new = reward[idx]\n",
        "            if not done[idx]:\n",
        "                Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n",
        "            target[idx][torch.argmax(action).item()] = Q_new\n",
        "\n",
        "        self.optimer.zero_grad()\n",
        "        loss = self.criterion(target,pred)\n",
        "        loss.backward()\n",
        " \n",
        "        self.optimer.step()"
      ],
      "metadata": {
        "id": "qGYPxjYkilaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_state(self, game):\n",
        "    head = game.snake[0]\n",
        "    point_l = Point(head.x - BLOCK_SIZE, head.y)\n",
        "    point_r = Point(head.x + BLOCK_SIZE, head.y)\n",
        "    point_u = Point(head.x, head.y - BLOCK_SIZE)\n",
        "    point_d = Point(head.x, head.y + BLOCK_SIZE)\n",
        " \n",
        "    dir_l = game.direction == Direction.LEFT\n",
        "    dir_r = game.direction == Direction.RIGHT\n",
        "    dir_u = game.direction == Direction.UP\n",
        "    dir_d = game.direction == Direction.DOWN\n",
        " \n",
        "    state = [\n",
        "  \n",
        "        (dir_u and game.is_collision(point_u))or\n",
        "        (dir_d and game.is_collision(point_d))or\n",
        "        (dir_l and game.is_collision(point_l))or\n",
        "        (dir_r and game.is_collision(point_r)),\n",
        " \n",
        "\n",
        "        (dir_u and game.is_collision(point_r))or\n",
        "        (dir_d and game.is_collision(point_l))or\n",
        "        (dir_u and game.is_collision(point_u))or\n",
        "        (dir_d and game.is_collision(point_d)),\n",
        " \n",
        "\n",
        "        (dir_u and game.is_collision(point_r))or\n",
        "        (dir_d and game.is_collision(point_l))or\n",
        "        (dir_r and game.is_collision(point_u))or\n",
        "        (dir_l and game.is_collision(point_d)),\n",
        " \n",
        "\n",
        "        dir_l,\n",
        "        dir_r,\n",
        "        dir_u,\n",
        "        dir_d,\n",
        " \n",
        "        # Food Location\n",
        "        game.food.x < game.head.x,  # food is in left\n",
        "        game.food.x > game.head.x,  # food is in right\n",
        "        game.food.y < game.head.y,  # food is up\n",
        "        game.food.y > game.head.y  # food is down\n",
        "    ]\n",
        "    return np.array(state, dtype=int)"
      ],
      "metadata": {
        "id": "5JrBSWNei67E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_action(self, state):\n",
        "    self.epsilon = 80 - self.n_game\n",
        "    final_move = [0, 0, 0]\n",
        "    if(random.randint(0, 200) < self.epsilon):\n",
        "        move = random.randint(0, 2)\n",
        "        final_move[move] = 1\n",
        "    else:\n",
        "        state0 = torch.tensor(state, dtype=torch.float).cuda()\n",
        "        prediction = self.model(state0).cuda()  # prediction by model\n",
        "        move = torch.argmax(prediction).item()\n",
        "        final_move[move] = 1\n",
        "    return final_move"
      ],
      "metadata": {
        "id": "WLsdUyGYjAHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_short_memory(self, state, action, reward, next_state, done):\n",
        "    self.trainer.train_step(state, action, reward, next_state, done)"
      ],
      "metadata": {
        "id": "8tLPN--JjCNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_long_memory(self):\n",
        "    if (len(self.memory) > BATCH_SIZE):\n",
        "        mini_sample = random.sample(self.memory, BATCH_SIZE)\n",
        "    else:\n",
        "        mini_sample = self.memory\n",
        "    states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
        "    self.trainer.train_step(states, actions, rewards, next_states, dones)"
      ],
      "metadata": {
        "id": "z45O3D2djOFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.model.load_state_dict(torch.load('PATH'))"
      ],
      "metadata": {
        "id": "DZ5azAcajPjE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}